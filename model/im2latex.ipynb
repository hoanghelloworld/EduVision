{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10817132,"sourceType":"datasetVersion","datasetId":6716006}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\nfrom collections import Counter\nfrom torch.utils.data import Dataset, DataLoader\n# ========================== #\n# 1️⃣ Đọc dữ liệu từ CSV\n# ========================== #\ndata_dir = \"/kaggle/input/im2latex-premium\"  # Thay bằng đường dẫn đúng\ntrain_path = os.path.join(data_dir, \"total_train.csv\")\nvalidate_path = os.path.join(data_dir, \"val2Ftotal_val.csv\")\n# test_path = os.path.join(data_dir, \"im2latex_test.csv\")\nimage_dir = os.path.join(data_dir, \"root/images\")  # Thư mục chứa ảnh\n\n# Đọc danh sách công thức từ im2latex_formulas.norm.csv\n# formulas_df = pd.read_csv(formulas_path)\n\n# Đọc tập huấn luyện, validation, test\ntrain_df = pd.read_csv(train_path)\nvalidate_df = pd.read_csv(validate_path)\n# test_df = pd.read_csv(test_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:00:06.566555Z","iopub.execute_input":"2025-02-22T10:00:06.566840Z","iopub.status.idle":"2025-02-22T10:00:27.858882Z","shell.execute_reply.started":"2025-02-22T10:00:06.566810Z","shell.execute_reply":"2025-02-22T10:00:27.858091Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageEnhance  # Add ImageEnhance here\nimport torch\nfrom torchvision import transforms\nfrom collections import Counter\nfrom torch.utils.data import Dataset, DataLoader\nclass Im2LatexDataset(Dataset):\n    def __init__(self, df, img_dir, max_len=200, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.max_len = max_len\n        self.transform = transform if transform else transforms.Compose([\n            transforms.Grayscale(num_output_channels=1),  # Convert to grayscale first\n            transforms.Resize((150, 700)),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: torch.where(x > 0.5, 1.0, 0.0)),  # Binarize image\n        ])\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx]['image_filename']\n        formula = self.df.iloc[idx]['latex']\n        formula = '<START> ' + formula + ' <END>'\n\n        # Load image\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert('L')  # Convert to grayscale explicitly\n\n        # Enhance contrast\n        enhancer = ImageEnhance.Contrast(image)\n        image = enhancer.enhance(2.0)\n\n        # Apply transforms\n        image = self.transform(image)\n\n        # Invert if needed (ensure dark text on light background)\n        if torch.mean(image) > 0.5:\n            image = 1 - image\n\n        return image, formula\n\n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:00:30.098479Z","iopub.execute_input":"2025-02-22T10:00:30.098776Z","iopub.status.idle":"2025-02-22T10:00:30.106573Z","shell.execute_reply.started":"2025-02-22T10:00:30.098753Z","shell.execute_reply":"2025-02-22T10:00:30.105671Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from collections import Counter\n\nclass Tokenizer:\n    def __init__(self, formulas, min_freq=5):\n        self.pad_token = '<PAD>'\n        self.start_token = '<START>'\n        self.end_token = '<END>'\n        self.unk_token = '<UNK>'\n\n        # Tạo vocabulary\n        word_counts = Counter()\n        for formula in formulas:\n            words = formula.split()\n            word_counts.update(words)\n\n        # Lọc từ có tần suất > min_freq\n        self.vocab = {self.pad_token: 0, self.start_token: 1,\n                     self.end_token: 2, self.unk_token: 3}\n\n        idx = len(self.vocab)\n        for word, count in word_counts.items():\n            if count >= min_freq:\n                self.vocab[word] = idx\n                idx += 1\n\n        self.reverse_vocab = {idx: word for word, idx in self.vocab.items()}\n\n    def encode(self, formula, max_len=200):\n        words = formula.split()\n        ids = [self.vocab.get(word, self.vocab[self.unk_token]) for word in words]\n\n        # Padding\n        if len(ids) < max_len:\n            ids = ids + [self.vocab[self.pad_token]] * (max_len - len(ids))\n        else:\n            ids = ids[:max_len]\n\n        return torch.LongTensor(ids)\n\n    def decode(self, ids):\n        words = [self.reverse_vocab[id.item()] for id in ids\n                if id.item() not in [self.vocab[self.pad_token]]]\n        return ' '.join(words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:00:38.567413Z","iopub.execute_input":"2025-02-22T10:00:38.567720Z","iopub.status.idle":"2025-02-22T10:00:38.575862Z","shell.execute_reply.started":"2025-02-22T10:00:38.567697Z","shell.execute_reply":"2025-02-22T10:00:38.574817Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=500):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n\n        efficientnet = mobilenet_v3_large(weights=None)\n\n        # Modify first conv layer to accept single channel input\n        efficientnet.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n\n        # Remove the classifier\n        self.features = efficientnet.features\n        self.linear = nn.Linear(960, embed_size)\n\n    def forward(self, images):\n        features = self.features(images)\n        features = features.permute(0, 2, 3, 1)  # [batch_size, height, width, channels]\n        features = features.view(features.size(0), -1, features.size(-1))  # [batch_size, seq_len, channels]\n        features = self.linear(features)\n        return features\nclass DecoderTransformer(nn.Module):\n    def __init__(self, embed_size, vocab_size, num_layers=6,\n                 nhead=8, dim_feedforward=1024, dropout=0.1):\n        super(DecoderTransformer, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.pos_encoder = PositionalEncoding(embed_size, dropout)\n\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embed_size,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout\n        )\n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer,\n            num_layers=num_layers\n        )\n\n        self.fc = nn.Linear(embed_size, vocab_size)\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def forward(self, enc_out, tgt, tgt_mask=None):\n        if tgt_mask is None:\n            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n\n        tgt = self.embedding(tgt)\n        tgt = self.pos_encoder(tgt)\n\n        output = self.transformer_decoder(\n            tgt.permute(1, 0, 2),\n            enc_out.permute(1, 0, 2),\n            tgt_mask=tgt_mask\n        )\n\n        output = output.permute(1, 0, 2)\n        output = self.fc(output)\n        return output\n\nclass Im2LatexModel(nn.Module):\n    def __init__(self, embed_size, vocab_size, **kwargs):\n        super(Im2LatexModel, self).__init__()\n        self.encoder = EncoderCNN(embed_size)\n        self.decoder = DecoderTransformer(embed_size, vocab_size, **kwargs)\n\n    def forward(self, images, formulas, formula_mask=None):\n        features = self.encoder(images)\n        outputs = self.decoder(features, formulas, formula_mask)\n        return outputs\n\n    def generate(self, image, start_token, end_token, max_len=200, beam_size=5):\n        with torch.no_grad():\n            # Encode image\n            features = self.encoder(image.unsqueeze(0))\n\n            # Initialize beam search\n            beams = [(torch.tensor([[start_token]], device=image.device), 0.0)]\n            completed_beams = []\n\n            for _ in range(max_len):\n                candidates = []\n\n                for seq, score in beams:\n                    if seq[0, -1].item() == end_token:\n                        completed_beams.append((seq, score))\n                        continue\n\n                    # Get predictions for next token\n                    out = self.decoder(features, seq)\n                    logits = out[:, -1, :]\n                    probs = F.log_softmax(logits, dim=-1)\n\n                    # Get top-k candidates\n                    values, indices = probs[0].topk(beam_size)\n                    for value, idx in zip(values, indices):\n                        new_seq = torch.cat([seq, idx.unsqueeze(0).unsqueeze(0)], dim=1)\n                        new_score = score + value.item()\n                        candidates.append((new_seq, new_score))\n\n                # Select top beam_size candidates\n                candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n                beams = candidates[:beam_size]\n\n                # Early stopping if all beams are completed\n                if len(completed_beams) >= beam_size:\n                    break\n\n            # Add incomplete beams to completed list\n            completed_beams.extend(beams)\n\n            # Return sequence with highest score\n            best_seq = max(completed_beams, key=lambda x: x[1])[0]\n\n            # Remove both start and end tokens\n            final_seq = []\n            for token in best_seq.squeeze(0)[1:].tolist():  # Skip start token\n                if token == end_token:  # Stop at end token\n                    break\n                final_seq.append(token)\n\n            return final_seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:00:52.222178Z","iopub.execute_input":"2025-02-22T10:00:52.222475Z","iopub.status.idle":"2025-02-22T10:00:52.241308Z","shell.execute_reply.started":"2025-02-22T10:00:52.222453Z","shell.execute_reply":"2025-02-22T10:00:52.240172Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch.cuda.amp as amp\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport time\nimport os\ndef train_model(model, train_loader, val_loader, tokenizer,\n               num_epochs=3, device='cuda'):\n\n    # Initialize wandb first\n    wandb.init(\n        project=\"im2latex_best_v1\",\n        settings=wandb.Settings(init_timeout=1200000),\n        config={\n            \"epochs\": num_epochs,\n            \"batch_size\": train_loader.batch_size * torch.cuda.device_count(), # Update batch size\n            \"learning_rate\": 0.0004,\n            \"architecture\": \"Efnet-Transformer\",\n            \"dataset_size\": len(train_loader.dataset),\n            \"num_gpus\": torch.cuda.device_count()\n        }\n    )\n\n    # Create checkpoint directory after wandb is initialized\n    checkpoint_dir = os.path.join(wandb.run.dir, \"checkpoints\")\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    scaler = amp.GradScaler()\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab['<PAD>'])\n\n    # Tăng learning rate theo số lượng GPU\n    lr = 0.0008\n    optimizer = Adam(model.parameters(), lr=lr)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5)\n    model = model.to(device)\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        epoch_start_time = time.time()\n\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n        for batch_idx, (images, formulas) in enumerate(train_loader):\n            images = images.to(device)\n            target_seqs = torch.stack([\n                tokenizer.encode(f) for f in formulas\n            ]).to(device)\n\n            input_seqs = target_seqs[:, :-1]\n            target_seqs = target_seqs[:, 1:]\n\n            with amp.autocast():\n                outputs = model(images, input_seqs)\n                loss = criterion(\n                    outputs.reshape(-1, outputs.size(-1)),\n                    target_seqs.reshape(-1)\n                )\n\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += loss.item()\n\n            if batch_idx % 100 == 0:\n                print(f\"\\rBatch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}\", end=\"\")\n                current_lr = optimizer.param_groups[0]['lr']\n                wandb.log({\n                    \"train_batch_loss\": loss.item(),\n                    \"learning_rate\": current_lr,\n                    \"epoch\": epoch,\n                    \"batch\": batch_idx,\n                    \"num_gpus\": torch.cuda.device_count()\n                }\n            )\n                scheduler.step(loss)\n\n        avg_train_loss = total_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n\n        with torch.no_grad():\n            with amp.autocast():\n                for images, formulas in val_loader:\n                    images = images.to(device)\n                    target_seqs = torch.stack([\n                        tokenizer.encode(f) for f in formulas\n                    ]).to(device)\n\n                    input_seqs = target_seqs[:, :-1]\n                    target_seqs = target_seqs[:, 1:]\n\n                    outputs = model(images, input_seqs)\n                    loss = criterion(\n                        outputs.reshape(-1, outputs.size(-1)),\n                        target_seqs.reshape(-1)\n                    )\n                    val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n        epoch_time = time.time() - epoch_start_time\n\n        print(f\"\\nTime: {epoch_time:.1f}s | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n        wandb.log({\n            \"train_loss\": avg_train_loss,\n            \"val_loss\": val_loss,\n            \"learning_rate\": optimizer.param_groups[0]['lr'],\n            \"epoch\": epoch\n        })\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            # Use wandb run directory for checkpoint path\n            checkpoint_path = os.path.join(\n                checkpoint_dir,\n                f'best_model.pth'\n            )\n            model_to_save = model.module if hasattr(model, 'module') else model\n            torch.save(model_to_save.state_dict(), checkpoint_path)\n            # Log best model artifact to wandb\n            artifact = wandb.Artifact(\n                name=f\"model-checkpoint-epoch{epoch}\",\n                type=\"model\",\n                description=f\"Model checkpoint from epoch {epoch} with val_loss {val_loss:.4f}\"\n            )\n                        # Add file to artifact after ensuring it exists\n            if os.path.exists(checkpoint_path):\n                artifact.add_file(checkpoint_path)\n                wandb.log_artifact(artifact)\n                print(f\"\\nSaved new best model checkpoint with val_loss: {val_loss:.4f}\")\n            else:\n                print(f\"\\nWarning: Failed to save checkpoint at {checkpoint_path}\")\n\n    wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:03:09.521815Z","iopub.execute_input":"2025-02-22T10:03:09.522514Z","iopub.status.idle":"2025-02-22T10:03:09.538785Z","shell.execute_reply.started":"2025-02-22T10:03:09.522480Z","shell.execute_reply":"2025-02-22T10:03:09.537938Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import wandb\nimport os\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport numpy as np\n\n# Set wandb API key\nos.environ[\"WANDB_API_KEY\"] = \"bab1bf4bf3d565e4e79b437fd9c484d62ac878c9\"\n\nif __name__ == \"__main__\":\n    # Initialize dataset and dataloader\n    train_dataset = Im2LatexDataset(train_df, image_dir)\n    val_dataset = Im2LatexDataset(validate_df, image_dir)\n\n    batch_size = 260\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=4\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4\n    )\n\n    # Initialize tokenizer\n    tokenizer = Tokenizer(train_df[\"latex\"].values)\n\n    def count_parameters(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    # Create the base model first\n    base_model = Im2LatexModel(\n        embed_size=256,\n        vocab_size=len(tokenizer.vocab),\n        num_layers=6,\n        nhead=8,\n        dim_feedforward=1024,\n        dropout=0.1\n    )\n\n    # Count and print parameters before wrapping with DataParallel\n    encoder_params = count_parameters(base_model.encoder)\n    decoder_params = count_parameters(base_model.decoder)\n    total_params = count_parameters(base_model)\n\n    print(f\"\\nModel Parameter Counts:\")\n    print(\"-\" * 40)\n    print(f\"Encoder: {encoder_params:,} parameters\")\n    print(f\"Decoder (Transformer): {decoder_params:,} parameters\")\n    print(f\"Total: {total_params:,} parameters\")\n    print(\"-\" * 40)\n\n    print(f\"\\nParameter Distribution:\")\n    print(f\"Encoder: {encoder_params/total_params*100:.1f}%\")\n    print(f\"Decoder: {decoder_params/total_params*100:.1f}%\")\n\n    # Now wrap the model with DataParallel\n    if torch.cuda.device_count() > 1:\n        print(f\"\\nUsing {torch.cuda.device_count()} GPUs!\")\n        model = nn.DataParallel(base_model)\n    else:\n        model = base_model\n\n    # Train model\n    train_model(model, train_loader, val_loader, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T10:03:13.268098Z","iopub.execute_input":"2025-02-22T10:03:13.268411Z","execution_failed":"2025-02-22T12:12:45.843Z"}},"outputs":[{"name":"stdout","text":"\nModel Parameter Counts:\n----------------------------------------\nEncoder: 3,217,680 parameters\nDecoder (Transformer): 8,044,833 parameters\nTotal: 11,262,513 parameters\n----------------------------------------\n\nParameter Distribution:\nEncoder: 28.6%\nDecoder: 71.4%\n\nUsing 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuyhoangak4\u001b[0m (\u001b[33mhoangvbck\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250222_100339-99nz8ksg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hoangvbck/im2latex_best_v1/runs/99nz8ksg' target=\"_blank\">avid-bird-1</a></strong> to <a href='https://wandb.ai/hoangvbck/im2latex_best_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hoangvbck/im2latex_best_v1' target=\"_blank\">https://wandb.ai/hoangvbck/im2latex_best_v1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hoangvbck/im2latex_best_v1/runs/99nz8ksg' target=\"_blank\">https://wandb.ai/hoangvbck/im2latex_best_v1/runs/99nz8ksg</a>"},"metadata":{}},{"name":"stderr","text":"<ipython-input-7-0417003af127>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-0417003af127>:54: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Batch [2000/13222] Loss: 0.4671","output_type":"stream"}],"execution_count":null}]}