{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageEnhance\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Load vocabulary from tokenizer.json\n",
    "def load_vocab():\n",
    "    with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        return data['vocab']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        from torchvision.models import mobilenet_v3_large\n",
    "\n",
    "        # Initialize MobileNetV3 without pretrained weights\n",
    "        mobilenet = mobilenet_v3_large(weights=None)\n",
    "        \n",
    "        # Modify first conv layer to accept single channel input\n",
    "        mobilenet.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        # Remove the classifier\n",
    "        self.features = mobilenet.features\n",
    "        self.linear = nn.Linear(960, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.features(images)\n",
    "        features = features.permute(0, 2, 3, 1)  # [batch_size, height, width, channels]\n",
    "        features = features.view(features.size(0), -1, features.size(-1))  # [batch_size, seq_len, channels]\n",
    "        features = self.linear(features)\n",
    "        return features\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, num_layers=6, nhead=8, dim_feedforward=1024, dropout=0.1):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, enc_out, tgt, tgt_mask=None):\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(\n",
    "            tgt.permute(1, 0, 2),\n",
    "            enc_out.permute(1, 0, 2),\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "        output = output.permute(1, 0, 2)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "class Im2LatexModel(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, **kwargs):\n",
    "        super(Im2LatexModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderTransformer(embed_size, vocab_size, **kwargs)\n",
    "\n",
    "    def forward(self, images, formulas, formula_mask=None):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, formulas, formula_mask)\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, image, start_token, end_token, max_len=200, beam_size=6):\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(image.unsqueeze(0))\n",
    "            # Initialize beam search\n",
    "            beams = [(torch.tensor([[start_token]], device=image.device), 0.0)]\n",
    "            completed_beams = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                candidates = []\n",
    "\n",
    "                for seq, score in beams:\n",
    "                    if seq[0, -1].item() == end_token:\n",
    "                        completed_beams.append((seq, score))\n",
    "                        continue\n",
    "\n",
    "                    # Get predictions for next token\n",
    "                    out = self.decoder(features, seq)\n",
    "                    logits = out[:, -1, :]\n",
    "                    probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "                    # Get top-k candidates\n",
    "                    values, indices = probs[0].topk(beam_size)\n",
    "                    for value, idx in zip(values, indices):\n",
    "                        new_seq = torch.cat([seq, idx.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                        new_score = score + value.item()\n",
    "                        candidates.append((new_seq, new_score))\n",
    "\n",
    "                # Select top beam_size candidates\n",
    "                candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "                beams = candidates[:beam_size]\n",
    "\n",
    "                # Early stopping if all beams are completed\n",
    "                if len(completed_beams) >= beam_size:\n",
    "                    break\n",
    "\n",
    "            # Add incomplete beams to completed list\n",
    "            completed_beams.extend(beams)\n",
    "\n",
    "            # Return sequence with highest score\n",
    "            best_seq = max(completed_beams, key=lambda x: x[1])[0]\n",
    "\n",
    "            # Remove both start and end tokens\n",
    "            final_seq = []\n",
    "            for token in best_seq.squeeze(0)[1:].tolist():  # Skip start token\n",
    "                if token == end_token:  # Stop at end token\n",
    "                    break\n",
    "                final_seq.append(token)\n",
    "\n",
    "            return final_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huyho\\AppData\\Local\\Temp\\ipykernel_5460\\1370446376.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Im2LatexModel(\n",
       "  (encoder): EncoderCNN(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv2dNormActivation(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=960, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): DecoderTransformer(\n",
       "    (embedding): Embedding(3361, 256)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=3361, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = load_vocab()\n",
    "reverse_vocab = {str(idx): word for word, idx in vocab.items()}\n",
    "    \n",
    "    # Initialize model\n",
    "model = Im2LatexModel(\n",
    "        embed_size=256,\n",
    "        vocab_size=len(vocab),\n",
    "        num_layers=6,\n",
    "        nhead=8,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "checkpoint_path = r\"C:\\Users\\huyho\\OneDrive\\Máy tính\\im2latex_llm\\best_model.pth\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple\n",
    "import torch.utils.mobile_optimizer as mobile_optimizer\n",
    "class TracedEncoder(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.encoder.eval()\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.encoder(image.unsqueeze(0))\n",
    "\n",
    "class TracedDecoder(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.decoder.eval()\n",
    "\n",
    "    def forward(self, features, tokens):\n",
    "        # Ensure tokens are long type\n",
    "        tokens = tokens.long()\n",
    "        return self.decoder(features, tokens)\n",
    "\n",
    "def convert_model():\n",
    "    \"\"\"Convert encoder and decoder separately\"\"\"\n",
    "    # Load original model\n",
    "    original_model = Im2LatexModel(\n",
    "        embed_size=256,\n",
    "        vocab_size=len(vocab),\n",
    "        num_layers=6,\n",
    "        nhead=8,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    checkpoint_path = \"best_model.pth\"\n",
    "    original_model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    original_model.eval()\n",
    "    \n",
    "    # Trace encoder\n",
    "    traced_encoder = TracedEncoder(original_model.encoder)\n",
    "    example_image = torch.randn(1, 150, 700)\n",
    "    traced_encoder = torch.jit.trace(traced_encoder, example_image)\n",
    "    # traced_encoder = mobile_optimizer.optimize_for_mobile(traced_encoder)\n",
    "\n",
    "    traced_decoder = TracedDecoder(original_model.decoder)\n",
    "    example_features = traced_encoder(example_image)\n",
    "    example_tokens = torch.zeros((1, 1), dtype=torch.long)\n",
    "    traced_decoder = torch.jit.trace(traced_decoder, (example_features, example_tokens))\n",
    "    # traced_decoder = mobile_optimizer.optimize_for_mobile(traced_decoder)\n",
    "    # Save models\n",
    "    # traced_encoder._save_for_lite_interpreter(\"encoder_traced.ptl\")\n",
    "    # traced_decoder._save_for_lite_interpreter(\"decoder_traced.ptl\")\n",
    "    traced_encoder.save(\"encoder_traced.ptl\")\n",
    "    traced_decoder.save(\"decoder_traced.ptl\")\n",
    "    \n",
    "    print(\"Models converted and saved successfully!\")\n",
    "    \n",
    "    return traced_encoder, traced_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huyho\\AppData\\Local\\Temp\\ipykernel_5460\\3837459150.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
      "c:\\Users\\huyho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\jit\\_trace.py:166: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  if a.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models converted and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder = convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model convert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((150, 700)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: torch.where(x > 0.5, 1.0, 0.0)),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('L')\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(2.0)\n",
    "    image = transform(image)\n",
    "    \n",
    "    if torch.mean(image) > 0.5:\n",
    "        image = 1 - image\n",
    "        \n",
    "    return image\n",
    "def decode_prediction(tokens, reverse_vocab):\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        word = reverse_vocab.get(str(token))\n",
    "        if word not in ['<PAD>', '<START>', '<END>', '<UNK>']:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_vocab():\n",
    "    with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        return data['vocab']\n",
    "vocab = load_vocab()\n",
    "reverse_vocab = {str(idx): word for word, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_traced_models(encoder, decoder, image, start_token, end_token, max_len=200, beam_size=5):\n",
    "    \"\"\"\n",
    "    Perform inference using beam search with the traced encoder and decoder\n",
    "    \n",
    "    Args:\n",
    "        encoder: Traced encoder model\n",
    "        decoder: Traced decoder model\n",
    "        image: Input image tensor\n",
    "        start_token: Token ID for sequence start\n",
    "        end_token: Token ID for sequence end\n",
    "        max_len: Maximum sequence length\n",
    "        beam_size: Size of beam for search\n",
    "        \n",
    "    Returns:\n",
    "        List of tokens representing the best sequence\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get image features\n",
    "        features = encoder(image)\n",
    "        \n",
    "        # Initialize beams: (sequence, score)\n",
    "        beams = [(torch.tensor([[start_token]], dtype=torch.long, device=image.device), 0.0)]\n",
    "        completed_beams = []\n",
    "        \n",
    "        # Beam search\n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            \n",
    "            # Expand each beam\n",
    "            for seq, score in beams:\n",
    "                # If sequence completed, add to completed beams\n",
    "                if seq[0, -1].item() == end_token:\n",
    "                    completed_beams.append((seq, score))\n",
    "                    continue\n",
    "                    \n",
    "                # Get predictions\n",
    "                out = decoder(features, seq)\n",
    "                logits = out[:, -1, :]\n",
    "                probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top-k candidates\n",
    "                values, indices = probs[0].topk(beam_size)\n",
    "                for value, idx in zip(values, indices):\n",
    "                    new_seq = torch.cat([seq, torch.tensor([[idx.item()]], dtype=torch.long, device=seq.device)], dim=1)\n",
    "                    new_score = score + value.item()\n",
    "                    candidates.append((new_seq, new_score))\n",
    "            \n",
    "            # Sort and select top-k candidates\n",
    "            candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "            beams = candidates[:beam_size]\n",
    "            \n",
    "            # Early stopping if enough complete sequences\n",
    "            if len(completed_beams) >= beam_size:\n",
    "                break\n",
    "        \n",
    "        # Add incomplete sequences to completed beams\n",
    "        completed_beams.extend(beams)\n",
    "        \n",
    "        # Select best sequence\n",
    "        best_seq = max(completed_beams, key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Convert to list of tokens (excluding start token)\n",
    "        return [token.item() for token in best_seq.squeeze(0)[1:] if token.item() != end_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.jit.load(\"encoder_traced.ptl\")\n",
    "decoder = torch.jit.load(\"decoder_traced.ptl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted LaTeX:\n",
      "\\displaystyle x = \\frac { - b \\pm \\sqrt { b ^ { 2 } - 4 a c } } { 2 a }\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance\n",
    "def test_converted_models(test_image_path: str):\n",
    "    \"\"\"Test the converted models with a sample image\"\"\"\n",
    "    encoder = torch.jit.load(\"encoder_traced.ptl\")\n",
    "    decoder = torch.jit.load(\"decoder_traced.ptl\")\n",
    "    \n",
    "    image = preprocess_image(test_image_path)\n",
    "    \n",
    "    tokens = inference_with_traced_models(\n",
    "        encoder, \n",
    "        decoder,\n",
    "        image,\n",
    "        start_token=vocab['<START>'],\n",
    "        end_token=vocab['<END>'],\n",
    "        beam_size=5 \n",
    "    )\n",
    "    \n",
    "    latex = decode_prediction(tokens, reverse_vocab)\n",
    "    print(f\"Predicted LaTeX:\\n{latex}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_converted_models(\"image_test/image copy.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
