{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtR1GhYwnLnu"
      },
      "source": [
        "# Train a LaTeX OCR model\n",
        "In this brief notebook I show how you can finetune/train an OCR model.\n",
        "\n",
        "I've opted to mix in handwritten data into the regular pdf LaTeX images. For that I started out with the released pretrained model and continued training on the slightly larger corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r396ah-Q3EQc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pix2tex[train] -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install entmax streamlit PyQt6 python-Levenshtein torchtext imagesize tqdm munch torch opencv_python_headless requests einops x_transformers transformers tokenizers numpy Pillow PyYAML pandas timm albumentations pyreadline3 pygments screeninfo pyside6 python-multipart uvicorn[standard] -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dZ4PLwkb3RIs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('LaTeX-OCR', exist_ok=True)\n",
        "\n",
        "# Change the current working directory\n",
        "os.chdir('LaTeX-OCR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cUsTlxXV3Mot"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python-headless in c:\\users\\huyho\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.0.80)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\huyho\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opencv-python-headless) (1.26.1)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uhLzh5vyaCaL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NguyenHuyHoang              2024-05-11 14:20:36  551.78\n",
            "\n",
            "[0] NVIDIA GeForce GTX 1650 | 50°C,  23 % |   404 /  4096 MB | NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) ?(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M) NGUYENHUYHOANG\\huyho(?M)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check what GPU we have\n",
        "gpustat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aAz37dDU21zu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM\n",
            "From (redirected): https://drive.google.com/uc?id=13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM&confirm=t&uuid=daca999c-b81a-444d-9b60-5afce51cbe44\n",
            "To: c:\\Users\\huyho\\Desktop\\LaTeX-OCR\\notebooks\\LaTeX-OCR\\dataset\\data\\crohme.zip\n",
            "100%|██████████| 59.8M/59.8M [00:18<00:00, 3.22MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ\n",
            "From (redirected): https://drive.google.com/uc?id=176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ&confirm=t&uuid=0b74cda9-6dca-4648-8a5e-25e7d5e5723d\n",
            "To: c:\\Users\\huyho\\Desktop\\LaTeX-OCR\\notebooks\\LaTeX-OCR\\dataset\\data\\pdf.zip\n",
            "100%|██████████| 284M/284M [01:22<00:00, 3.45MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D\n",
            "To: c:\\Users\\huyho\\Desktop\\LaTeX-OCR\\notebooks\\LaTeX-OCR\\dataset\\data\\pdfmath.txt\n",
            "100%|██████████| 36.6M/36.6M [00:10<00:00, 3.34MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "os.chdir('LaTeX-OCR')\n",
        "# Tạo các thư mục nếu chưa tồn tại\n",
        "os.makedirs('dataset/data', exist_ok=True)\n",
        "os.makedirs('image', exist_ok=True)\n",
        "\n",
        "# Tải các file từ Google Drive\n",
        "gdown.download('https://drive.google.com/uc?id=13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM', 'dataset/data/crohme.zip', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ', 'dataset/data/pdf.zip', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D', 'dataset/data/pdfmath.txt', quiet=False)\n",
        "\n",
        "# Giải nén các file zip\n",
        "with zipfile.ZipFile('dataset/data/crohme.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset/data')\n",
        "\n",
        "with zipfile.ZipFile('dataset/data/pdf.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset/data')\n",
        "\n",
        "# Tách dữ liệu handwritten thành tập validation và tập train\n",
        "os.makedirs('dataset/valimages', exist_ok=True)\n",
        "\n",
        "image_files = os.listdir('dataset/data/images')\n",
        "val_files = random.sample(image_files, 1000)\n",
        "\n",
        "for file in val_files:\n",
        "    shutil.move(os.path.join('dataset/data/images', file), 'dataset/valimages')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMuIqRIqG-8"
      },
      "source": [
        "Now we generate the datasets. We can string multiple datasets together to get one large lookup table. The only thing saved in these pkl files are image sizes, image location and the ground truth latex code. That way we can serve batches of images with the same dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JebcEarl-g6"
      },
      "outputs": [],
      "source": [
        "python -m pix2tex.dataset.dataset -i dataset/data/images dataset/data/train -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/train.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Orutb37xHD"
      },
      "outputs": [],
      "source": [
        "python -m pix2tex.dataset.dataset -i dataset/data/valimages dataset/data/val -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/val.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vow2NnpHmWt0"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (192739755.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install wandb\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# If using wandb\n",
        "pip install wandb \n",
        "# you can cancel this if you don't wan't to use it or don't have a W&B acc.\n",
        "#!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
            "  0 97.3M    0 25003    0     0   7689      0  3:41:20  0:00:03  3:41:17  118k\n",
            "  0 97.3M    0  395k    0     0  94834      0  0:17:56  0:00:04  0:17:52  322k\n",
            "  1 97.3M    1 1283k    0     0   244k      0  0:06:47  0:00:05  0:06:42  581k\n",
            "  2 97.3M    2 2418k    0     0   367k      0  0:04:31  0:00:06  0:04:25  684k\n",
            "  3 97.3M    3 3649k    0     0   484k      0  0:03:25  0:00:07  0:03:18  814k\n",
            "  6 97.3M    6 6889k    0     0   834k      0  0:01:59  0:00:08  0:01:51 1372k\n",
            " 11 97.3M   11 11.4M    0     0  1266k      0  0:01:18  0:00:09  0:01:09 2273k\n",
            " 17 97.3M   17 17.1M    0     0  1708k      0  0:00:58  0:00:10  0:00:48 3242k\n",
            " 23 97.3M   23 22.4M    0     0  2040k      0  0:00:48  0:00:11  0:00:37 4398k\n",
            " 28 97.3M   28 27.8M    0     0  2325k      0  0:00:42  0:00:12  0:00:30 5256k\n",
            " 34 97.3M   34 33.4M    0     0  2583k      0  0:00:38  0:00:13  0:00:25 5474k\n",
            " 40 97.3M   40 38.9M    0     0  2799k      0  0:00:35  0:00:14  0:00:21 5636k\n",
            " 46 97.3M   46 44.8M    0     0  3010k      0  0:00:33  0:00:15  0:00:18 5690k\n",
            " 50 97.3M   50 49.3M    0     0  3112k      0  0:00:32  0:00:16  0:00:16 5524k\n",
            " 56 97.3M   56 55.1M    0     0  3274k      0  0:00:30  0:00:17  0:00:13 5597k\n",
            " 60 97.3M   60 59.1M    0     0  3316k      0  0:00:30  0:00:18  0:00:12 5258k\n",
            " 66 97.3M   66 64.9M    0     0  3448k      0  0:00:28  0:00:19  0:00:09 5283k\n",
            " 73 97.3M   73 71.2M    0     0  3600k      0  0:00:27  0:00:20  0:00:07 5402k\n",
            " 79 97.3M   79 77.5M    0     0  3738k      0  0:00:26  0:00:21  0:00:05 5772k\n",
            " 85 97.3M   85 82.8M    0     0  3812k      0  0:00:26  0:00:22  0:00:04 5670k\n",
            " 90 97.3M   90 88.4M    0     0  3897k      0  0:00:25  0:00:23  0:00:02 6015k\n",
            " 95 97.3M   95 93.4M    0     0  3945k      0  0:00:25  0:00:24  0:00:01 5880k\n",
            "100 97.3M  100 97.3M    0     0  3996k      0  0:00:24  0:00:24 --:--:-- 5701k\n"
          ]
        }
      ],
      "source": [
        "# download the weights we want to fine tune\n",
        "!curl -L -o weights.pth https://github.com/lukas-blecher/LaTeX-OCR/releases/download/v0.0.1/weights.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OnsNCLp84QSY"
      },
      "outputs": [],
      "source": [
        "# generate colab specific config (set 'debug' to true if wandb is not used)\n",
        "!echo {backbone_layers: [2, 3, 7], betas: [0.9, 0.999], batchsize: 10, bos_token: 1, channels: 1, data: dataset/data/train.pkl, debug: true, decoder_args: {'attn_on_attn': true, 'cross_attend': true, 'ff_glu': true, 'rel_pos_bias': false, 'use_scalenorm': false}, dim: 256, encoder_depth: 4, eos_token: 2, epochs: 50, gamma: 0.9995, heads: 8, id: null, load_chkpt: 'weights.pth', lr: 0.001, lr_step: 30, max_height: 192, max_seq_len: 512, max_width: 672, min_height: 32, min_width: 32, model_path: checkpoints, name: mixed, num_layers: 4, num_tokens: 8000, optimizer: Adam, output_path: outputs, pad: false, pad_token: 0, patch_size: 16, sample_freq: 2000, save_freq: 1, scheduler: StepLR, seed: 42, temperature: 0.2, test_samples: 5, testbatchsize: 20, tokenizer: dataset/tokenizer.json, valbatches: 100, valdata: dataset/data/val.pkl} > colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c8NU5j2k3z36"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (4209187608.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m pix2tex.train --config colab.yaml\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "python -m pix2tex.train --config colab.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LaTeX-OCR training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
